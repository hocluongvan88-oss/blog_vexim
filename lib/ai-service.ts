import Groq from "groq-sdk"

// Initialize Groq client
let groq: Groq | null = null

try {
  if (!process.env.GROQ_API_KEY) {
    console.error("[v0] GROQ_API_KEY is not set in environment variables")
  } else {
    groq = new Groq({
      apiKey: process.env.GROQ_API_KEY,
    })
    console.log("[v0] Groq client initialized successfully")
  }
} catch (error) {
  console.error("[v0] Error initializing Groq client:", error)
}

export interface AIConfig {
  model: string
  maxTokens: number
  temperature: number
  systemPrompt: string
}

export interface KnowledgeChunk {
  id: string
  chunk_text: string
  document_title: string
  category: string
}

export interface AIResponse {
  message: string
  confidence: number
  sources: string[]
  shouldHandover: boolean
  handoverReason?: string
}

/**
 * T·∫°o embedding cho text (c·∫ßn OpenAI ho·∫∑c alternative)
 * ƒê√¢y l√† placeholder - b·∫°n c·∫ßn implement v·ªõi OpenAI API ho·∫∑c local model
 */
export async function createEmbedding(text: string): Promise<number[]> {
  // TODO: Implement v·ªõi OpenAI API ho·∫∑c local embedding model
  // Hi·ªán t·∫°i return mock embedding
  console.log("[v0] Creating embedding for:", text.substring(0, 100))
  return Array(1536).fill(0)
}

/**
 * T√¨m ki·∫øm t√†i li·ªáu li√™n quan t·ª´ knowledge base
 */
export async function searchKnowledge(
  query: string,
  topK: number = 5,
  supabase: any
): Promise<KnowledgeChunk[]> {
  try {
    // T·∫°o embedding cho query
    const queryEmbedding = await createEmbedding(query)

    // T√¨m ki·∫øm similarity (c·∫ßn pgvector extension)
    // Hi·ªán t·∫°i s·ª≠ d·ª•ng full-text search thay th·∫ø
    const { data, error } = await supabase
      .from("knowledge_chunks")
      .select(
        `
        id,
        chunk_text,
        knowledge_documents!inner(title, category)
      `
      )
      .textSearch("chunk_text", query, {
        type: "websearch",
        config: "english",
      })
      .limit(topK)

    if (error) throw error

    return (
      data?.map((item: any) => ({
        id: item.id,
        chunk_text: item.chunk_text,
        document_title: item.knowledge_documents.title,
        category: item.knowledge_documents.category,
      })) || []
    )
  } catch (error) {
    console.error("[v0] Error searching knowledge:", error)
    return []
  }
}

/**
 * X√¢y d·ª±ng context t·ª´ knowledge chunks
 */
function buildContext(chunks: KnowledgeChunk[]): string {
  if (chunks.length === 0) return ""

  const context = chunks
    .map(
      (chunk, index) =>
        `[T√†i li·ªáu ${index + 1}: ${chunk.document_title}]\n${chunk.chunk_text}`
    )
    .join("\n\n")

  return `\n\nTh√¥ng tin tham kh·∫£o:\n${context}`
}

/**
 * Ph√¢n t√≠ch intent v√† x√°c ƒë·ªãnh c·∫ßn chuy·ªÉn sang agent kh√¥ng
 */
function analyzeIntent(
  message: string,
  aiResponse: string
): { confidence: number; shouldHandover: boolean; reason?: string } {
  const lowConfidenceKeywords = [
    "kh√¥ng ch·∫Øc",
    "c√≥ th·ªÉ",
    "kh√¥ng r√µ",
    "xin l·ªói",
    "kh√¥ng hi·ªÉu",
  ]
  const urgentKeywords = [
    "kh·∫©n c·∫•p",
    "g·∫•p",
    "ngay",
    "urgent",
    "nhanh",
  ]
  const complexKeywords = [
    "t∆∞ v·∫•n chi ti·∫øt",
    "b√°o gi√° c·ª• th·ªÉ",
    "h·ª£p ƒë·ªìng",
    "th·ªèa thu·∫≠n",
    "k√Ω k·∫øt",
  ]

  let confidence = 0.8 // Default confidence

  // Check for low confidence indicators
  const hasLowConfidence = lowConfidenceKeywords.some((keyword) =>
    aiResponse.toLowerCase().includes(keyword)
  )
  if (hasLowConfidence) confidence = 0.3

  // Check for urgent request
  const isUrgent = urgentKeywords.some((keyword) =>
    message.toLowerCase().includes(keyword)
  )
  if (isUrgent) {
    return {
      confidence: 0.2,
      shouldHandover: true,
      reason: "Y√™u c·∫ßu kh·∫©n c·∫•p c·∫ßn x·ª≠ l√Ω ngay",
    }
  }

  // Check for complex request
  const isComplex = complexKeywords.some((keyword) =>
    message.toLowerCase().includes(keyword)
  )
  if (isComplex) {
    return {
      confidence: 0.4,
      shouldHandover: true,
      reason: "Y√™u c·∫ßu ph·ª©c t·∫°p c·∫ßn chuy√™n gia t∆∞ v·∫•n",
    }
  }

  return {
    confidence,
    shouldHandover: confidence < 0.5,
    reason:
      confidence < 0.5 ? "AI kh√¥ng ƒë·ªß tin c·∫≠y ƒë·ªÉ tr·∫£ l·ªùi" : undefined,
  }
}

/**
 * T·∫°o response t·ª´ AI v·ªõi RAG
 */
export async function generateAIResponse(
  message: string,
  conversationHistory: Array<{ role: string; content: string }>,
  config: AIConfig,
  supabase: any,
  ragEnabled: boolean = true
): Promise<AIResponse> {
  try {
    // Check if Groq is initialized
    if (!groq) {
      throw new Error("Groq client is not initialized. Please check GROQ_API_KEY environment variable.")
    }

    let knowledgeChunks: KnowledgeChunk[] = []
    let context = ""

    // RAG: T√¨m ki·∫øm knowledge base
    if (ragEnabled) {
      knowledgeChunks = await searchKnowledge(message, 5, supabase)
      context = buildContext(knowledgeChunks)
    }

    // X√¢y d·ª±ng messages cho Groq
    const messages: any[] = [
      {
        role: "system",
        content: config.systemPrompt + context,
      },
      ...conversationHistory,
      {
        role: "user",
        content: message,
      },
    ]

    console.log("[v0] Calling Groq API with model:", config.model)

    // G·ªçi Groq API
    const completion = await groq.chat.completions.create({
      model: config.model,
      messages,
      temperature: config.temperature,
      max_tokens: config.maxTokens,
    })

    const aiMessage = completion.choices[0]?.message?.content || ""
    console.log("[v0] Received AI response:", aiMessage.substring(0, 100))

    // Ph√¢n t√≠ch intent v√† confidence
    const analysis = analyzeIntent(message, aiMessage)

    // N·∫øu AI kh√¥ng t·ª± tin, g·ª£i √Ω chuy·ªÉn sang agent
    let finalMessage = aiMessage
    if (analysis.shouldHandover) {
      finalMessage += `\n\nüí° ƒê·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n, b·∫°n c√≥ th·ªÉ y√™u c·∫ßu k·∫øt n·ªëi v·ªõi chuy√™n gia c·ªßa ch√∫ng t√¥i.`
    }

    return {
      message: finalMessage,
      confidence: analysis.confidence,
      sources: knowledgeChunks.map((c) => c.document_title),
      shouldHandover: analysis.shouldHandover,
      handoverReason: analysis.reason,
    }
  } catch (error) {
    console.error("[v0] Error generating AI response:", error)

    return {
      message:
        "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá hotline ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ tr·ª±c ti·∫øp.",
      confidence: 0.0,
      sources: [],
      shouldHandover: true,
      handoverReason: "L·ªói h·ªá th·ªëng AI",
    }
  }
}

/**
 * Load AI config t·ª´ database
 */
export async function loadAIConfig(supabase: any): Promise<AIConfig> {
  try {
    const { data, error } = await supabase
      .from("ai_config")
      .select("key, value")
      .in("key", [
        "groq_model",
        "max_tokens",
        "temperature",
        "system_prompt",
      ])

    if (error) throw error

    const config: any = {}
    data?.forEach((item: any) => {
      config[item.key] = item.value
    })

    return {
      model: config.groq_model || "llama-3.3-70b-versatile",
      maxTokens: parseInt(config.max_tokens) || 1024,
      temperature: parseFloat(config.temperature) || 0.7,
      systemPrompt:
        config.system_prompt ||
        "B·∫°n l√† AI assistant c·ªßa Vexim Global, chuy√™n t∆∞ v·∫•n v·ªÅ xu·∫•t nh·∫≠p kh·∫©u.",
    }
  } catch (error) {
    console.error("[v0] Error loading AI config:", error)
    return {
      model: "llama-3.3-70b-versatile",
      maxTokens: 1024,
      temperature: 0.7,
      systemPrompt:
        "B·∫°n l√† AI assistant c·ªßa Vexim Global, chuy√™n t∆∞ v·∫•n v·ªÅ xu·∫•t nh·∫≠p kh·∫©u, FDA, MFDS, GACC v√† c√°c d·ªãch v·ª• li√™n quan.",
    }
  }
}
